import os
import json
import torch
import dgl
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import random
import fasttext.util

from dgl.nn import GraphConv
from dgl.data.utils import save_graphs, load_graphs
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    roc_auc_score,
    accuracy_score,
    recall_score,
    f1_score,
    ndcg_score,
)
from torch.utils.data import DataLoader, TensorDataset
from tabulate import tabulate
from scipy import stats

from model import GNNModel
import config
import utils
import shutil
import time


def load_data(filename):

    # å¦‚æœç‰¹å¾ä¸å›¾æ•°æ®æœ‰æ›´æ–°ï¼Œè¯·åˆ é™¤ä»¥ä¸‹æ–‡ä»¶ï¼Œç¡®ä¿åˆ é™¤æ—§çš„graph_data.binã€features_file.ptå’Œuuid_to_index.ptæ–‡ä»¶ã€‚
    # è¿™æ ·ï¼Œä¸‹æ¬¡è¿è¡Œload_dataå‡½æ•°æ—¶ï¼Œä¼šæ ¹æ®æ›´æ–°åçš„æ•°æ®é‡æ–°ç”Ÿæˆå’Œä¿å­˜è¿™äº›æ–‡ä»¶ã€‚
    graph_data_file = "./data/graph_data.bin"
    features_file = "./data/features_file.pt"
    uuid_to_index_file = "./data/uuid_to_index.pt"

    # æ¯æ¬¡éƒ½è½½å…¥å†…å­˜
    with open(filename, "r", encoding="utf-8") as file:
        data = json.load(file)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™ç›´æ¥åŠ è½½
    if (
        os.path.exists(graph_data_file)
        and os.path.exists(features_file)
        and os.path.exists(uuid_to_index_file)
    ):
        graphs, _ = load_graphs(graph_data_file)
        g = graphs[0]  # åŠ è½½å›¾
        features_scaled = torch.load(features_file)  # åŠ è½½ç‰¹å¾
        uuid_to_index = torch.load(uuid_to_index_file)  # åŠ è½½uuidåˆ°ç´¢å¼•çš„æ˜ å°„
        return g, features_scaled, data["papers"], data["edges"], uuid_to_index

    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ‰§è¡ŒåŸæœ‰çš„æ•°æ®åŠ è½½å’Œå¤„ç†æµç¨‹

    # åŠ è½½é¢„è®­ç»ƒçš„fastTextæ¨¡å‹
    fasttext.util.download_model("zh", if_exists="ignore")  # 'zh'ä»£è¡¨ä¸­æ–‡
    ft = fasttext.load_model("cc.zh.300.bin")

    # åˆ›å»ºä»UUIDåˆ°æ•´æ•°ç´¢å¼•çš„æ˜ å°„
    uuid_to_index = {paper["id"]: idx for idx, paper in enumerate(data["papers"])}

    # ä½¿ç”¨æ˜ å°„è½¬æ¢edgesä¸­çš„UUIDåˆ°æ•´æ•°ç´¢å¼•
    edges_src = [
        uuid_to_index[edge["source"]]
        for edge in data["edges"]
        for target in edge["target"]
    ]
    edges_dst = [
        uuid_to_index[target] for edge in data["edges"] for target in edge["target"]
    ]

    # print('uuid_to_index')
    # print(uuid_to_index)
    # print('edges_src')
    # print(edges_src)
    # print('edges_dst')
    # print(edges_dst)

    num_papers = len(data["papers"])
    g = dgl.graph((edges_src, edges_dst))
    g.add_nodes(
        num_papers - g.number_of_nodes()
    )  # ç¡®ä¿å›¾ä¸­æœ‰æ­£ç¡®æ•°é‡çš„èŠ‚ç‚¹(è¡¥å…¨æ— è¾¹èŠ‚ç‚¹)
    g = dgl.add_self_loop(g)

    # ä¿å­˜ä¸€ä»½DGLçš„äºŒè¿›åˆ¶æ ¼å¼ç”¨äºé¢„æµ‹è„šæœ¬
    save_graphs(graph_data_file, [g])

    # å¤„ç†å¹´ä»½ç‰¹å¾
    years = np.array([[paper["year"]] for paper in data["papers"]])

    # å°†å­—ç¬¦ä¸²ç±»å‹è½¬æ¢ä¸ºæµ®ç‚¹æ•°
    years = years.astype(np.float32)

    # ç„¶åè½¬æ¢ä¸ºtorch.FloatTensor
    years = torch.FloatTensor(years)

    # ç”Ÿæˆæ ‡é¢˜å’Œæ‘˜è¦å’ŒæœŸåˆŠçš„è¯å‘é‡
    title_embeddings = np.array(
        [ft.get_sentence_vector(paper["title"]) for paper in data["papers"]]
    )
    abstract_embeddings = np.array(
        [
            ft.get_sentence_vector(paper["abstract"].replace("\n", ""))
            for paper in data["papers"]
        ]
    )
    journal_embeddings = np.array(
        [ft.get_sentence_vector(paper["journal"]) for paper in data["papers"]]
    )

    # å¤„ç†ä½œè€…ä¿¡æ¯ï¼ˆç®€å•ç¤ºä¾‹ï¼šä½¿ç”¨OneHotEncoderï¼‰
    authors_list = [
        ",".join(paper["authors"]) for paper in data["papers"]
    ]  # å°†ä½œè€…åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    encoder = OneHotEncoder(sparse=False)
    author_features = encoder.fit_transform(np.array(authors_list).reshape(-1, 1))

    # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
    features = np.concatenate(
        [
            years,
            title_embeddings,
            abstract_embeddings,
            journal_embeddings,
            author_features,
        ],
        axis=1,
    )
    # features = np.concatenate([years], axis=1)
    features = torch.FloatTensor(features)

    # å®ä¾‹åŒ–å½’ä¸€åŒ–å·¥å…·
    scaler = StandardScaler()

    # å¯¹å¹´ä»½ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–
    years_scaled = scaler.fit_transform(years)

    # å¯¹åµŒå…¥å‘é‡è¿›è¡Œå½’ä¸€åŒ–
    title_embeddings_scaled = scaler.fit_transform(title_embeddings)
    abstract_embeddings_scaled = scaler.fit_transform(abstract_embeddings)
    journal_embeddings_scaled = scaler.fit_transform(journal_embeddings)

    # æ‹¼æ¥æ‰€æœ‰å½’ä¸€åŒ–åçš„ç‰¹å¾
    features_scaled = np.concatenate(
        [
            years_scaled,
            title_embeddings_scaled,
            abstract_embeddings_scaled,
            journal_embeddings_scaled,
            author_features,
        ],
        axis=1,
    )
    features_scaled = torch.FloatTensor(features_scaled)

    # æ³¨æ„ï¼šä½œè€…ç‰¹å¾(author_features)é€šå¸¸ä¸éœ€è¦å½’ä¸€åŒ–ï¼Œå› ä¸ºå®ƒæ˜¯ç‹¬çƒ­ç¼–ç çš„

    # ä¿å­˜ç‰¹å¾æ•°æ®ä¸º .pt æ–‡ä»¶ç”¨äºé¢„æµ‹è„šæœ¬
    # torch.save(features, features_file)
    torch.save(features_scaled, features_file)

    # ä¿å­˜ä»UUIDåˆ°å›¾èŠ‚ç‚¹ç´¢å¼•çš„æ˜ å°„æ•°æ®ä¸º .pt æ–‡ä»¶ç”¨äºé¢„æµ‹è„šæœ¬
    uuid_to_index = {paper["id"]: idx for idx, paper in enumerate(data["papers"])}
    torch.save(uuid_to_index, uuid_to_index_file)

    # å°†æ˜ å°„è½¬æ¢ä¸ºJSONæ ¼å¼çš„å­—ç¬¦ä¸²
    uuid_to_index_json = json.dumps(uuid_to_index, indent=4)

    # å°†JSONå­—ç¬¦ä¸²ä¿å­˜åˆ°æ–‡ä»¶ï¼Œä»¥å¤‡å‚è€ƒå¯¹ç…§
    with open("./data/uuid_to_index.json", "w") as json_file:
        json_file.write(uuid_to_index_json)

    # print('title_embeddings')
    # print(title_embeddings)

    # print('author_features')
    # print(author_features)

    # return g, features, data['papers'], data['edges'], uuid_to_index
    return g, features_scaled, data["papers"], data["edges"], uuid_to_index


def construct_positive_negative_edges(
    g, edges, uuid_to_index, test_size=0.2, val_size=0.1
):
    # æ„é€ æ­£æ ·æœ¬è¾¹ï¼Œä½¿ç”¨uuid_to_indexæ˜ å°„
    pos_edges = []
    for edge in edges:
        src = uuid_to_index[edge["source"]]
        targets = [uuid_to_index[t] for t in edge["target"]]
        for dst in targets:
            pos_edges.append((src, dst))

    # æ„é€ è´Ÿæ ·æœ¬
    all_nodes = list(range(g.number_of_nodes()))
    neg_edges = []
    tried_pairs = set()

    while len(neg_edges) < len(pos_edges):
        u = random.choice(all_nodes)
        v = random.choice(all_nodes)
        if u == v or (u, v) in tried_pairs:
            continue

        tried_pairs.add((u, v))
        tried_pairs.add((v, u))

        if not g.has_edges_between([u], [v]).bool().item():
            neg_edges.append((u, v))

    # print('pos_edges@construct_positive_negative_edges')
    # print(pos_edges[:10])
    # print('neg_edges@construct_positive_negative_edges')
    # print(neg_edges[:10])

    # åˆ†å‰²æ•°æ®é›†
    pos_train, pos_temp = train_test_split(
        pos_edges, test_size=test_size + val_size, random_state=42
    )
    pos_val, pos_test = train_test_split(
        pos_temp, test_size=test_size / (test_size + val_size), random_state=42
    )

    neg_train, neg_temp = train_test_split(
        neg_edges, test_size=test_size + val_size, random_state=42
    )
    neg_val, neg_test = train_test_split(
        neg_temp, test_size=test_size / (test_size + val_size), random_state=42
    )

    # è½¬æ¢ä¸ºTensor
    pos_train_tensor = torch.tensor(pos_train, dtype=torch.long)
    pos_val_tensor = torch.tensor(pos_val, dtype=torch.long)
    pos_test_tensor = torch.tensor(pos_test, dtype=torch.long)
    neg_train_tensor = torch.tensor(neg_train, dtype=torch.long)
    neg_val_tensor = torch.tensor(neg_val, dtype=torch.long)
    neg_test_tensor = torch.tensor(neg_test, dtype=torch.long)

    return (
        pos_train_tensor,
        pos_val_tensor,
        pos_test_tensor,
        neg_train_tensor,
        neg_val_tensor,
        neg_test_tensor,
    )


def prepare_dataloader(pos_edges, neg_edges, batch_size):

    # print('pos_edges@prepare_dataloader')
    # print(pos_edges[:10])
    # print('neg_edges@prepare_dataloader()')
    # print(neg_edges[:10])

    # åˆ›å»ºDataLoader
    dataset = TensorDataset(pos_edges, neg_edges)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    return loader


def train(model, train_loader, optimizer, g, features):
    device = model.device  # ä»æ¨¡å‹ä¸­è·å–å½“å‰è®¾å¤‡
    model.train()
    total_loss = 0
    for batch in train_loader:

        # print('batch')
        # print(batch)

        pos_edges, neg_edges = batch

        pos_edges = pos_edges.to(device)  # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        neg_edges = neg_edges.to(device)  # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡

        optimizer.zero_grad()

        # è·å–å›¾çš„èŠ‚ç‚¹è¡¨ç¤º
        h = model(g, features)

        # print('pos_edges@train')
        # print(pos_edges)

        # print('neg_edges@train')
        # print(neg_edges)

        # ä½¿ç”¨predict_linksæ–¹æ³•è®¡ç®—æ­£è´Ÿæ ·æœ¬çš„é¢„æµ‹å¾—åˆ†
        pos_score = model.predict_links(h, pos_edges)
        neg_score = model.predict_links(h, neg_edges)

        # åˆ›å»ºæ ‡ç­¾å¹¶è®¡ç®—æŸå¤±
        labels = torch.cat(
            [
                torch.ones(pos_score.size(0), device=device),
                torch.zeros(neg_score.size(0), device=device),
            ]
        )
        predictions = torch.cat([pos_score, neg_score])

        # åœ¨è®¡ç®—æŸå¤±ä¹‹å‰ï¼Œç¡®ä¿labelsçš„å°ºå¯¸ä¸predictionsåŒ¹é…
        labels = labels.unsqueeze(1)

        loss = F.binary_cross_entropy(predictions, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"Average Loss: {avg_loss}")
    return avg_loss


def ndcg_score(y_true, y_scores, k=20):
    actual = np.take(y_true, np.argsort(y_scores)[::-1])[:k]
    ideal = np.sort(y_true)[::-1][:k]
    dcg = np.sum(actual / np.log2(np.arange(2, k + 2)))
    idcg = np.sum(ideal / np.log2(np.arange(2, k + 2)))
    return dcg / idcg if idcg > 0 else 0


def evaluate(model, loader, g, features, k):
    device = next(model.parameters()).device

    model.eval()
    y_true = []
    y_scores = []  # ç”¨äºNDCGè®¡ç®—çš„åˆ†æ•°

    with torch.no_grad():
        for batch in loader:
            pos_edges, neg_edges = batch
            pos_edges, neg_edges = pos_edges.to(device), neg_edges.to(device)

            h = model(g.to(device), features.to(device))
            pos_score = model.predict_links(h, pos_edges)
            neg_score = model.predict_links(h, neg_edges)

            y_true.extend([1] * len(pos_score) + [0] * len(neg_score))
            y_scores.extend(
                pos_score.squeeze().cpu().numpy().tolist()
                + neg_score.squeeze().cpu().numpy().tolist()
            )

    auc = roc_auc_score(y_true, y_scores)
    accuracy = accuracy_score(y_true, [1 if score > 0.5 else 0 for score in y_scores])
    recall = recall_score(y_true, [1 if score > 0.5 else 0 for score in y_scores])
    f1 = f1_score(y_true, [1 if score > 0.5 else 0 for score in y_scores])
    ndcg = ndcg_score(np.array(y_true), np.array(y_scores), k=k)

    print(
        f"Evaluation - AUC: {auc:.4f}, Accuracy: {accuracy:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, NDCG@{k}: {ndcg:.4f}"
    )
    return auc, accuracy, recall, f1, ndcg


def main():
    # ç¡®å®šæ‰§è¡Œçš„è®¾å¤‡
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    g, features, papers, edges, uuid_to_index = load_data("./data/paper.json")

    # ç§»åŠ¨å›¾å’Œç‰¹å¾åˆ°æŒ‡å®šè®¾å¤‡
    g = g.to(device)
    features = features.to(device)

    pos_train, pos_val, pos_test, neg_train, neg_val, neg_test = (
        construct_positive_negative_edges(g, edges, uuid_to_index)
    )

    train_loader = prepare_dataloader(pos_train, neg_train, config.batch_size)
    val_loader = prepare_dataloader(pos_val, neg_val, config.batch_size)
    test_loader = prepare_dataloader(pos_test, neg_test, config.batch_size)

    model = GNNModel(
        in_feats=features.shape[1],
        hidden_feats=config.hidden_feats,
        num_layers=config.num_layers,
        dropout_rate=config.dropout_rate,
        num_heads=config.num_heads,
    ).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)

    # å°è¯•åŠ è½½å·²æœ‰æ¨¡å‹
    model_checkpoint_path = "./model/model_checkpoint.pth"
    if os.path.exists(model_checkpoint_path):
        model.load_state_dict(torch.load(model_checkpoint_path, map_location=device))
        print("Model loaded and will continue training.")

        # Create a backup of the existing model checkpoint
        timestamp = time.strftime("%Y%m%d%H%M%S", time.localtime())
        backup_path = f"./model/backup/model_checkpoint-{timestamp}.pth"
        shutil.copyfile(model_checkpoint_path, backup_path)
        print(f"Model checkpoint backed up as {backup_path}")
    else:
        print("No existing model found. Starting training from scratch.")

    best_val_auc = 0.0  # å‡è®¾ä»¥AUCä¸ºåŸºå‡†è¿›è¡Œæ—©åœ
    patience_counter = 0
    patience = config.patience  # è®¾ç½®è€å¿ƒå€¼ï¼Œä¾‹å¦‚10ä¸ªepoch

    training_logs = [
        [
            "Epoch",
            "Loss",
            "Val AUC",
            "Val Accuracy",
            "Val Recall",
            "Val F1",
            "Val NDCG@" + str(config.top_k),
            "å¤‡æ³¨ï¼ˆCommentsï¼‰",
        ]
    ]

    for epoch in range(config.epoch_count):
        train_loss = train(model, train_loader, optimizer, g, features)
        auc, accuracy, recall, f1, ndcg = evaluate(
            model, val_loader, g, features, config.top_k
        )

        print(
            f"ğŸ¤– Epoch {epoch+1}, Current AUC: {auc:.4f}, Best AUC: {best_val_auc:.4f}, Patience Counter: {patience_counter}"
        )

        if auc > best_val_auc:
            best_val_auc = auc
            patience_counter = 0  # é‡ç½®è€å¿ƒè®¡æ•°å™¨
            torch.save(model.state_dict(), model_checkpoint_path)  # ä¿å­˜æœ€å¥½çš„æ¨¡å‹
            print(
                f"ğŸ‰ Model saved at epoch {epoch+1} with improvement in AUC: {auc:.4f}"
            )
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f"âŒ Early stopping triggered at epoch {epoch+1}")
            break  # æå‰ç»ˆæ­¢è®­ç»ƒ

        # åŠ¨æ€æ·»åŠ å¤‡æ³¨
        if epoch < config.epoch_count * 0.25:
            comment = "åˆæ¬¡è®­ç»ƒï¼ˆInitial Trainingï¼‰"
        elif epoch < config.epoch_count * 0.5:
            comment = "æ€§èƒ½æå‡ï¼ˆPerformance Improvementï¼‰"
        elif epoch < config.epoch_count * 0.75:
            comment = "æ¥è¿‘æ”¶æ•›ï¼ˆApproaching Convergenceï¼‰"
        else:
            comment = "è®­ç»ƒç»“æŸï¼ˆEnd of Trainingï¼‰"

        training_logs.append(
            [
                epoch + 1,
                f"{train_loss:.4f}",
                f"{auc:.4f}",
                f"{accuracy:.4f}",
                f"{recall:.4f}",
                f"{f1:.4f}",
                f"{ndcg:.4f}",
                comment,
            ]
        )

    # è®­ç»ƒç»“æŸåï¼Œè¾“å‡ºæ‰€æœ‰è®­ç»ƒæ—¥å¿—
    print(tabulate(training_logs, headers="firstrow", tablefmt="grid"))

    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
    model.load_state_dict(torch.load(model_checkpoint_path))
    print("Loaded best model for testing.")

    auc, accuracy, recall, f1, ndcg = evaluate(
        model, test_loader, g, features, config.top_k
    )
    print(
        f"ğŸ’¡ Test AUC: {auc:.4f}, Test Accuracy: {accuracy:.4f}, Test Recall: {recall:.4f}, Test F1: {f1:.4f}, Test NDCG@{str(config.top_k)}: {ndcg:.4f}"
    )


if __name__ == "__main__":
    main()
